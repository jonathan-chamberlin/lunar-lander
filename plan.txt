(DONE) Now use this prompt and also paste in the toggle heading below: I was trying to follow the youtube tutorial but the guy sped through stuff and i didnâ€™t understand why he was doing what he was doing, so i need your help walking me through each step so i understand this and learn this new skill of reinforcement learning on a continuous environment. Here is the project description from the youtube video i want to build
- project description from youtube video
    
    In this tutorial we will code a deep deterministic policy gradient (DDPG) agent in Pytorch, to beat the continuous lunar lander environment.
    
    DDPG combines the best of Deep Q Learning and Actor Critic Methods into an algorithm that can solve environments with continuous action spaces. We will have an actor network that learns the (deterministic) policy, coupled with a critic network to learn the action-value functions. We will make use of a replay buffer to maximize sample efficiency, as well as target networks to assist in algorithm convergence and stability.
    
    To deal with the explore exploit dilemma, we will introduce noise into the agent's action choice function. This noise is the Ornstein Uhlenbeck noise that models temporal correlations of brownian motion.
    
    Keep in mind that the performance you see is from an agent that is still in training mode, i.e. it still has some noise in its action. A fully trained agent in evaluation mode will perform even better. You can fix this up in the code by adding a parameter to the choose action function, and omitting the noise if you pass in a variable to indicate you are in evaluation mode.
    
- (DONE)Read this claude plan overview from the most recent response and answer each question: https://claude.ai/chat/baebe5cc-226c-4d00-b55c-42542815f019
- (DONE)  Install claude code: https://code.claude.com/docs/en/overview


(DONE) Rewatch this video about actor critic network: https://www.youtube.com/watch?v=oydExwuuUCw

(DONE) Based on that video, ask claude if it makes more sense to create a function or class for each network. To me with my limited knowledge about the uses of classes it seems like functions would be better, where all parameters for each are passed as argurments.

(DONE) Read my most recent message in my claude convo on browser. Then look at my psuedocode on @main.py, because that's my best understanding of how to actually implement this code

(IN PROGRESS) Make it so the simulation actually can get successful landings, because although it's now descending upright, it is not printing the SUCCESS message.

(DONE) make it so the action includes some OUActionNoise

(DONE) Read most recent claude prompt and response

(DONE) The lander seems ot be tipping over a lot more, so i need to investigate why. Paste codebase into claude to ask why.

(DONE) Make it so I can render every 10th run

(DONE) Make it so I can render an arbitrary list of runs_to_render

(DONE) Run this message in claude code:
Now i want to be able to time my simulation to benchmark how fast it is to do 100 runs. How can I do that? I want a value on inputs called timing: bool. If it's true, the timer starts, and if it's true, the last thing to print on the terminal is the time it took.

(DONE) Time how long a simulation takes to render all 100 runs. Record it: IT TOOK Total simulation time: 427.07 seconds

(DONE) Time how long a simulation takes to render no runs. Record it: IT TOOK Total simulation time: 59.03 seconds

(DONE) Speed up my simulation by vectorizing the environments. Total simulation time: 33.73 seconds

Make it so the agent doesn't just learn to shoot the agent upwards and doesn't attempt to land

Keep running my simulation to ensure it actually hits the benchmark of 200 reward for 100 episodes in a row

Next to speed up the code, ask what are other ways I can speed up this code.

Then say "Sometimes if you change your code to be less readable, it can be faster. Find high impact examples of where this could be applied, especially on complex operations that run every frame 

